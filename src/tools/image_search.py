"""ì´ë¯¸ì§€ ê²€ìƒ‰ ë„êµ¬ - Serper.dev + Playwright ì¹´ì¹´ì˜¤ë§µ ì—°ë™

1. ì´ë¯¸ì§€ â†’ Serper Google Lens â†’ ì‹ë‹¹ëª… ì¶”ì¶œ
2. ì‹ë‹¹ëª… â†’ ì¹´ì¹´ì˜¤ë§µ API â†’ place_id íšë“
3. place_id â†’ Playwright ì¹´ì¹´ì˜¤ë§µ í¬ë¡¤ë§ â†’ ë©”ë‰´/ê°€ê²© ì „ì²´ ì¶”ì¶œ
"""

import os
import re
import base64
import asyncio

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (ëª¨ë“ˆ ì„í¬íŠ¸ ì‹œ ìë™ ë¡œë“œ)
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass
from pathlib import Path
from typing import Optional, Dict, Any, List, Union
from collections import Counter
from langchain_core.tools import tool

try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False

try:
    from bs4 import BeautifulSoup
    BS4_AVAILABLE = True
except ImportError:
    BS4_AVAILABLE = False

try:
    from playwright.async_api import async_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False


class SerperImageSearcher:
    """Serper.devë¥¼ í™œìš©í•œ ì´ë¯¸ì§€ ê²€ìƒ‰ê¸°

    Google Lens + í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì§€ì›
    ë¡œì»¬ íŒŒì¼ë„ ì§€ì› (ì„ì‹œ ì—…ë¡œë“œ)
    SerpAPI ëŒ€ë¹„ 15ë°° ì €ë ´, ë¬´ë£Œ 2,500íšŒ/ì›”
    """

    def __init__(self, api_key: Optional[str] = None):
        self.serper_key = api_key or os.getenv("SERPER_API_KEY")
        self.serpapi_key = os.getenv("SERPAPI_KEY")
        self.api_key = self.serper_key  # í…ìŠ¤íŠ¸ ê²€ìƒ‰ìš©
        self.lens_url = "https://google.serper.dev/lens"
        self.search_url = "https://google.serper.dev/search"
        self.serpapi_url = "https://serpapi.com/search"

    def _apply_exif_orientation(self, file_path: str) -> str:
        """
        EXIF orientationì„ ì ìš©í•œ ì´ë¯¸ì§€ë¥¼ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥

        Args:
            file_path: ì›ë³¸ ì´ë¯¸ì§€ ê²½ë¡œ

        Returns:
            íšŒì „ ì ìš©ëœ ì´ë¯¸ì§€ ê²½ë¡œ (ì„ì‹œ íŒŒì¼ ë˜ëŠ” ì›ë³¸)
        """
        try:
            from PIL import Image, ExifTags

            img = Image.open(file_path)

            # EXIF orientation ì°¾ê¸°
            orientation_key = None
            for key in ExifTags.TAGS.keys():
                if ExifTags.TAGS[key] == 'Orientation':
                    orientation_key = key
                    break

            exif = img._getexif()
            if exif and orientation_key and orientation_key in exif:
                orientation = exif[orientation_key]

                # íšŒì „ í•„ìš”í•œ ê²½ìš°ë§Œ ì²˜ë¦¬
                if orientation == 3:
                    img = img.rotate(180, expand=True)
                elif orientation == 6:
                    img = img.rotate(270, expand=True)
                elif orientation == 8:
                    img = img.rotate(90, expand=True)
                else:
                    return file_path  # íšŒì „ ë¶ˆí•„ìš”

                # ìºì‹œ ë¬´íš¨í™”: í”½ì…€ í•˜ë‚˜ ìˆ˜ì • (í˜¸ìŠ¤íŒ… ì„œë¹„ìŠ¤ ìºì‹œ ë°©ì§€)
                import random
                pixels = img.load()
                x, y = img.width - 1, img.height - 1
                r, g, b = pixels[x, y][:3] if len(pixels[x, y]) >= 3 else (pixels[x, y], pixels[x, y], pixels[x, y])
                pixels[x, y] = (r, g, (b + random.randint(1, 5)) % 256)

                # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
                import tempfile
                temp_file = tempfile.NamedTemporaryFile(suffix='.jpeg', delete=False)
                img.save(temp_file.name, format='JPEG', quality=90)
                print(f"ğŸ“ EXIF íšŒì „ ì ìš©ë¨ (orientation={orientation})")
                return temp_file.name

        except Exception as e:
            print(f"  - EXIF ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

        return file_path

    def upload_image(self, file_path: str) -> Optional[str]:
        """
        ë¡œì»¬ ì´ë¯¸ì§€ë¥¼ ì„ì‹œ í˜¸ìŠ¤íŒ… ì„œë¹„ìŠ¤ì— ì—…ë¡œë“œ

        Args:
            file_path: ë¡œì»¬ ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ

        Returns:
            ì—…ë¡œë“œëœ ì´ë¯¸ì§€ì˜ ê³µê°œ URL (ì‹¤íŒ¨ì‹œ None)
        """
        if not os.path.exists(file_path):
            return None

        # EXIF orientation ì ìš©
        file_path = self._apply_exif_orientation(file_path)

        # ì—¬ëŸ¬ ì„œë¹„ìŠ¤ ìˆœì°¨ ì‹œë„ (litterbox ìš°ì„  - ìºì‹œ ì—†ìŒ)
        upload_services = [
            self._upload_to_litterbox,
            self._upload_to_imgbb,
            self._upload_to_freeimage,
        ]

        for upload_func in upload_services:
            try:
                url = upload_func(file_path)
                if url:
                    return url
            except Exception as e:
                print(f"  - {upload_func.__name__} ì‹¤íŒ¨: {e}")
                continue

        return None

    def _upload_to_imgbb(self, file_path: str) -> Optional[str]:
        """imgbb.comì— ì—…ë¡œë“œ (ë¬´ë£Œ, ë¹ ë¦„)"""
        with open(file_path, 'rb') as f:
            image_data = base64.b64encode(f.read()).decode()

        # ê³µê°œ API ì—”ë“œí¬ì¸íŠ¸ (API í‚¤ ì—†ì´ ì‚¬ìš©)
        response = requests.post(
            'https://api.imgbb.com/1/upload',
            data={
                'key': 'da2d77ea2fc52e04d4e62a6d3906f48f',  # ê³µê°œ ë°ëª¨ í‚¤
                'image': image_data,
                'expiration': 600,  # 10ë¶„ í›„ ë§Œë£Œ
            },
            timeout=30
        )

        if response.status_code == 200:
            data = response.json()
            if data.get('success'):
                return data['data']['url']
        return None

    def _upload_to_freeimage(self, file_path: str) -> Optional[str]:
        """freeimage.hostì— ì—…ë¡œë“œ"""
        with open(file_path, 'rb') as f:
            response = requests.post(
                'https://freeimage.host/api/1/upload',
                data={'key': '6d207e02198a847aa98d0a2a901485a5'},  # ê³µê°œ API í‚¤
                files={'source': f},
                timeout=30
            )

        if response.status_code == 200:
            data = response.json()
            if data.get('status_code') == 200:
                return data['image']['url']
        return None

    def _upload_to_litterbox(self, file_path: str) -> Optional[str]:
        """litterbox.catbox.moeì— ì—…ë¡œë“œ (ì„ì‹œ íŒŒì¼ìš©)"""
        with open(file_path, 'rb') as f:
            response = requests.post(
                'https://litterbox.catbox.moe/resources/internals/api.php',
                data={'reqtype': 'fileupload', 'time': '1h'},
                files={'fileToUpload': f},
                timeout=60
            )

        if response.status_code == 200:
            url = response.text.strip()
            if url.startswith('http'):
                return url
        return None

    def is_local_file(self, path: str) -> bool:
        """ê²½ë¡œê°€ ë¡œì»¬ íŒŒì¼ì¸ì§€ í™•ì¸"""
        return os.path.exists(path) or (
            not path.startswith('http://') and
            not path.startswith('https://') and
            Path(path).suffix.lower() in ['.jpg', '.jpeg', '.png', '.gif', '.webp']
        )

    def get_image_url(self, image_source: str) -> Optional[str]:
        """
        ì´ë¯¸ì§€ ì†ŒìŠ¤(URL ë˜ëŠ” ë¡œì»¬ ê²½ë¡œ)ì—ì„œ ê³µê°œ URL íšë“

        Args:
            image_source: ì´ë¯¸ì§€ URL ë˜ëŠ” ë¡œì»¬ íŒŒì¼ ê²½ë¡œ

        Returns:
            ê³µê°œ ì ‘ê·¼ ê°€ëŠ¥í•œ ì´ë¯¸ì§€ URL
        """
        # ì´ë¯¸ URLì¸ ê²½ìš°
        if image_source.startswith('http://') or image_source.startswith('https://'):
            return image_source

        # ë¡œì»¬ íŒŒì¼ì¸ ê²½ìš° ì—…ë¡œë“œ
        if os.path.exists(image_source):
            print(f"ğŸ“¤ ë¡œì»¬ íŒŒì¼ ì—…ë¡œë“œ ì¤‘: {image_source}")
            uploaded_url = self.upload_image(image_source)
            if uploaded_url:
                print(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {uploaded_url}")
                return uploaded_url
            else:
                print("âŒ ì—…ë¡œë“œ ì‹¤íŒ¨")

        return None

    def search_with_lens(self, image_url: str) -> Dict[str, Any]:
        """
        Google Lensë¡œ ì´ë¯¸ì§€ ê²€ìƒ‰ (SerpAPI ìš°ì„ , Serper í´ë°±)

        Args:
            image_url: ê²€ìƒ‰í•  ì´ë¯¸ì§€ì˜ ê³µê°œ URL

        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not REQUESTS_AVAILABLE:
            return {"error": "requests ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

        # 1. SerpAPI ì‹œë„ (ìš°ì„ )
        if self.serpapi_key:
            try:
                params = {
                    "engine": "google_lens",
                    "url": image_url,
                    "api_key": self.serpapi_key,
                    "hl": "ko",
                    "country": "kr"
                }
                response = requests.get(self.serpapi_url, params=params, timeout=30)
                response.raise_for_status()
                result = response.json()

                # SerpAPI í˜•ì‹ ë³€í™˜
                visual_matches = result.get("visual_matches", [])
                if visual_matches:
                    return {
                        "visual_matches": visual_matches,
                        "text": result.get("text_results", []),
                        "knowledge_graph": result.get("knowledge_graph", {})
                    }
            except Exception as e:
                print(f"SerpAPI ì‹¤íŒ¨, Serperë¡œ í´ë°±: {e}")

        # 2. Serper í´ë°±
        if not self.serper_key:
            return {"error": "API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. SERPAPI_KEY ë˜ëŠ” SERPER_API_KEYë¥¼ .envì— ì¶”ê°€í•´ì£¼ì„¸ìš”."}

        headers = {
            "X-API-KEY": self.serper_key,
            "Content-Type": "application/json"
        }
        data = {
            "url": image_url,
            "gl": "kr",
            "hl": "ko"
        }

        try:
            response = requests.post(self.lens_url, headers=headers, json=data, timeout=30)
            response.raise_for_status()
            result = response.json()

            return {
                "visual_matches": result.get("organic", []),
                "text": [],
                "knowledge_graph": {}
            }
        except requests.Timeout:
            return {"error": "API ìš”ì²­ ì‹œê°„ ì´ˆê³¼ (30ì´ˆ)"}
        except requests.RequestException as e:
            return {"error": f"API ìš”ì²­ ì‹¤íŒ¨: {str(e)}"}

    def search_reverse_image(self, image_url: str) -> Dict[str, Any]:
        """
        SerperëŠ” reverse imageë¥¼ lensë¡œ í†µí•© ì œê³µ
        lens ê²°ê³¼ ë°˜í™˜
        """
        return self.search_with_lens(image_url)

    def search_with_combined(self, image_url: str) -> Dict[str, Any]:
        """
        ì—¬ëŸ¬ ê²€ìƒ‰ ë°©ë²•ì„ ì¡°í•©í•˜ì—¬ ìµœìƒì˜ ê²°ê³¼ ë°˜í™˜

        ìˆœì„œ: Google Lens â†’ Reverse Image â†’ (ê²°ê³¼ ë³‘í•©)
        """
        results = {
            "lens": None,
            "reverse": None,
            "combined": True
        }

        # 1. Google Lens ì‹œë„
        lens_result = self.search_with_lens(image_url)
        if "error" not in lens_result:
            results["lens"] = lens_result

        # 2. Reverse Image ì‹œë„
        reverse_result = self.search_reverse_image(image_url)
        if "error" not in reverse_result:
            results["reverse"] = reverse_result

        # 3. ê²°ê³¼ ë³‘í•©
        if results["lens"] or results["reverse"]:
            return self._merge_results(results["lens"], results["reverse"])

        return {"error": "Google Lensì™€ Reverse Image ëª¨ë‘ ê²°ê³¼ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."}

    def fetch_page_content(self, url: str) -> Optional[str]:
        """
        ê²€ìƒ‰ ê²°ê³¼ ë§í¬ì˜ ì‹¤ì œ í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜´
        ë„¤ì´ë²„ ë¸”ë¡œê·¸ëŠ” ëª¨ë°”ì¼ ë²„ì „ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì ‘ê·¼
        """
        import re

        # ë„¤ì´ë²„ ë¸”ë¡œê·¸ -> ëª¨ë°”ì¼ ë²„ì „ìœ¼ë¡œ ë³€í™˜
        if 'blog.naver.com' in url:
            url = url.replace('blog.naver.com', 'm.blog.naver.com')

        headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X)'
        }

        try:
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code != 200:
                return None

            # HTMLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = response.text
            text = re.sub(r'<script[^>]*>.*?</script>', '', text, flags=re.DOTALL)
            text = re.sub(r'<style[^>]*>.*?</style>', '', text, flags=re.DOTALL)
            text = re.sub(r'<[^>]+>', ' ', text)
            text = ' '.join(text.split())

            return text[:5000]  # ì•ë¶€ë¶„ë§Œ
        except:
            return None

    def extract_menu_from_page(self, page_text: str) -> List[str]:
        """
        í˜ì´ì§€ í…ìŠ¤íŠ¸ì—ì„œ ë©”ë‰´ëª… í›„ë³´ ì¶”ì¶œ
        """
        import re

        # ê°€ê²© íŒ¨í„´ ì£¼ë³€ì˜ ë©”ë‰´ëª… ì°¾ê¸°
        menu_with_price = re.findall(r'([ê°€-í£a-zA-Z\s]{2,20})\s*[\d,]+ì›', page_text)

        # "~ë¼ëŠ” ìŒì‹", "~ë¥¼ ì£¼ë¬¸" ë“±ì˜ íŒ¨í„´
        menu_patterns = [
            r'([ê°€-í£]{2,15})(?:ë¼ëŠ”|ë¼ê³  í•˜ëŠ”)\s*(?:ìŒì‹|ë©”ë‰´)',
            r'([ê°€-í£]{2,15})(?:ë¥¼|ì„)\s*(?:ì£¼ë¬¸|ì‹œì¼°|ë¨¹ì—ˆ)',
            r'ì£¼ë¬¸[:\s]*([ê°€-í£]{2,15})',
        ]

        menus = list(menu_with_price)
        for pattern in menu_patterns:
            matches = re.findall(pattern, page_text)
            menus.extend(matches)

        # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
        cleaned = []
        for m in menus:
            m = m.strip()
            if len(m) >= 2 and m not in cleaned:
                # ì¼ë°˜ ë‹¨ì–´ ì œì™¸
                if m not in ['ë§›ìˆëŠ”', 'ì •ë§', 'ì§„ì§œ', 'ì˜¤ëŠ˜', 'ì—¬ê¸°', 'ì´ë²ˆ', 'ë‹¤ìŒ']:
                    cleaned.append(m)

        return cleaned[:10]

    def search_text(self, query: str) -> Dict[str, Any]:
        """
        Serper í…ìŠ¤íŠ¸ ê²€ìƒ‰ìœ¼ë¡œ ì¶”ê°€ ì •ë³´ íšë“
        """
        if not self.api_key:
            return {"error": "SERPER_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

        headers = {
            "X-API-KEY": self.api_key,
            "Content-Type": "application/json"
        }
        data = {
            "q": query,
            "gl": "kr",
            "hl": "ko"
        }

        try:
            response = requests.post(self.search_url, headers=headers, json=data, timeout=30)
            response.raise_for_status()
            result = response.json()

            # Serper í˜•ì‹ì„ ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            return {
                "organic_results": result.get("organic", []),
                "answer_box": result.get("answerBox", {})
            }
        except requests.RequestException as e:
            return {"error": f"API ìš”ì²­ ì‹¤íŒ¨: {str(e)}"}

    def get_menu_price_info(self, brand: str, menu: str) -> Optional[Dict[str, Any]]:
        """
        ë¸Œëœë“œì™€ ë©”ë‰´ëª…ìœ¼ë¡œ ê°€ê²©/ë©”íƒ€ ì •ë³´ ê²€ìƒ‰
        """
        import re

        query = f"{brand} {menu} ê°€ê²© ë©”ë‰´"
        result = self.search_text(query)

        if "error" in result:
            return None

        info = {
            "prices": [],
            "menu_items": [],
            "calories": None,
            "description": None
        }

        # Organic Resultsì—ì„œ ê°€ê²© ì •ë³´ ì¶”ì¶œ
        organic = result.get("organic_results", [])
        for item in organic[:5]:
            snippet = item.get("snippet", "")
            title = item.get("title", "")
            text = f"{title} {snippet}"

            # ê°€ê²© íŒ¨í„´
            price_matches = re.findall(r'(\d{1,2}[,.]?\d{3})\s*ì›', text)
            for price in price_matches:
                price_str = f"{price}ì›"
                if price_str not in info["prices"]:
                    info["prices"].append(price_str)

            # ì¹¼ë¡œë¦¬ íŒ¨í„´
            cal_match = re.search(r'(\d+)\s*(?:kcal|ì¹¼ë¡œë¦¬)', text, re.I)
            if cal_match and not info["calories"]:
                info["calories"] = f"{cal_match.group(1)}kcal"

        # Answer Boxê°€ ìˆìœ¼ë©´ í™œìš©
        answer_box = result.get("answer_box", {})
        if answer_box:
            info["description"] = answer_box.get("snippet") or answer_box.get("answer")

        return info if info["prices"] or info["calories"] else None

    def _merge_results(self, lens: Optional[Dict], reverse: Optional[Dict]) -> Dict[str, Any]:
        """ë‘ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë³‘í•©"""
        merged = {
            "knowledge_graph": {},
            "visual_matches": [],
            "text": [],
            "related_searches": [],
            "image_results": []
        }

        if lens:
            merged["knowledge_graph"] = lens.get("knowledge_graph", {})
            merged["visual_matches"] = lens.get("visual_matches", [])
            merged["text"] = lens.get("text", [])
            merged["related_searches"] = lens.get("related_searches", [])

        if reverse:
            # Reverse Image ê²°ê³¼ ì¶”ê°€
            merged["image_results"] = reverse.get("image_results", [])
            if not merged["knowledge_graph"]:
                merged["knowledge_graph"] = reverse.get("knowledge_graph", {})

        return merged

    def extract_food_info(self, lens_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        Google Lens ê²°ê³¼ì—ì„œ ìŒì‹ ì •ë³´ ì¶”ì¶œ

        Returns:
            {
                "identified": str,        # ì¸ì‹ëœ ìŒì‹ëª…
                "brand": str,             # ë¸Œëœë“œ/ê°€ê²Œëª…
                "menu_name": str,         # ì •í™•í•œ ë©”ë‰´ëª…
                "price": str,             # ê°€ê²© ì •ë³´
                "description": str,       # ì„¤ëª…
                "related_results": list,  # ê´€ë ¨ ê²€ìƒ‰ ê²°ê³¼
                "text_in_image": list,    # ì´ë¯¸ì§€ ë‚´ í…ìŠ¤íŠ¸
                "keywords": list,         # ê´€ë ¨ í‚¤ì›Œë“œ
            }
        """
        if "error" in lens_result:
            return {"error": lens_result["error"]}

        extracted = {
            "identified": None,
            "brand": None,
            "menu_name": None,
            "price": None,
            "description": None,
            "related_results": [],
            "text_in_image": [],
            "keywords": [],
            "sources": [],
            "raw_titles": []  # ë©”ë‰´ëª… ì¶”ì¶œìš©
        }

        # 1. Knowledge Graph (ê°€ì¥ ì •í™•í•œ ì •ë³´)
        kg = lens_result.get("knowledge_graph", {})
        if kg:
            extracted["identified"] = kg.get("title")
            extracted["description"] = kg.get("description")

        # 2. Visual Matches (ìœ ì‚¬ ì´ë¯¸ì§€ ë§¤ì¹­)
        visual_matches = lens_result.get("visual_matches", [])
        for match in visual_matches[:10]:
            title = match.get("title", "")
            extracted["related_results"].append({
                "title": title,
                "source": match.get("source", ""),
                "link": match.get("link", ""),
                "snippet": match.get("snippet", "")
            })
            if title:
                extracted["raw_titles"].append(title)
            if match.get("source"):
                extracted["sources"].append(match.get("source"))

        # 3. Image Results (Reverse Imageì—ì„œ)
        image_results = lens_result.get("image_results", [])
        for img in image_results[:5]:
            title = img.get("title", "")
            extracted["related_results"].append({
                "title": title,
                "source": img.get("source", ""),
                "link": img.get("link", ""),
                "snippet": img.get("snippet", "")
            })
            if title:
                extracted["raw_titles"].append(title)

        # 4. ì´ë¯¸ì§€ ë‚´ í…ìŠ¤íŠ¸
        text_results = lens_result.get("text", [])
        for text_item in text_results[:5]:
            if text_item.get("text"):
                extracted["text_in_image"].append(text_item["text"])

        # 5. ê´€ë ¨ ê²€ìƒ‰ì–´
        related_searches = lens_result.get("related_searches", [])
        for search in related_searches[:5]:
            if search.get("query"):
                extracted["keywords"].append(search["query"])

        # 6. ë¸Œëœë“œ/ë©”ë‰´ëª…/ê°€ê²© ì¶”ì¶œ
        self._extract_detailed_info(extracted)

        return extracted

    def _extract_detailed_info(self, extracted: Dict[str, Any]):
        """
        ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê°€ê²© ì •ë³´ë§Œ ì¶”ì¶œ
        ë¸Œëœë“œ/ë©”ë‰´ ë¶„ì„ì€ LLMì—ê²Œ ë§¡ê¹€
        """
        import re

        all_text = " ".join(extracted["raw_titles"] + extracted["text_in_image"] + extracted["keywords"])

        # ê°€ê²© ì¶”ì¶œ (ìˆ«ì+ì› íŒ¨í„´)
        price_matches = re.findall(r'(\d{1,3}[,.]?\d{3})\s*ì›', all_text)
        if price_matches:
            # ì¤‘ë³µ ì œê±°
            unique_prices = list(dict.fromkeys(price_matches))
            extracted["price"] = ", ".join([f"{p}ì›" for p in unique_prices[:3]])

    def format_result(self, extracted: Dict[str, Any]) -> str:
        """ì¶”ì¶œëœ ì •ë³´ë¥¼ ì½ê¸° ì¢‹ì€ í˜•ì‹ìœ¼ë¡œ í¬ë§· (LLM í•´ì„ìš©)"""
        if "error" in extracted:
            return f"âŒ ì˜¤ë¥˜: {extracted['error']}"

        parts = []

        # Knowledge Graph ì¸ì‹ ê²°ê³¼
        if extracted.get("identified"):
            parts.append(f"ğŸ“Œ Google ì¸ì‹: {extracted['identified']}")

        if extracted.get("description"):
            parts.append(f"ğŸ“ ì„¤ëª…: {extracted['description']}")

        # ê´€ë ¨ ê²€ìƒ‰ ê²°ê³¼ (í•µì‹¬ ì •ë³´ - LLMì´ í•´ì„)
        if extracted.get("related_results"):
            parts.append("\nğŸ” ê²€ìƒ‰ ê²°ê³¼ (ë¸Œëœë“œ/ë©”ë‰´ ì •ë³´ í¬í•¨):")
            for i, result in enumerate(extracted["related_results"][:7], 1):
                title = result.get("title", "")
                source = result.get("source", "")
                if title:
                    line = f"  {i}. {title}"
                    if source:
                        line += f" [{source}]"
                    parts.append(line)

        # ì´ë¯¸ì§€ ë‚´ í…ìŠ¤íŠ¸ (ê°„íŒ, ë©”ë‰´íŒ ë“±)
        if extracted.get("text_in_image"):
            texts = ", ".join(extracted["text_in_image"])
            parts.append(f"\nğŸ“„ ì´ë¯¸ì§€ í…ìŠ¤íŠ¸: {texts}")

        # ê´€ë ¨ í‚¤ì›Œë“œ
        if extracted.get("keywords"):
            keywords = ", ".join(extracted["keywords"][:5])
            parts.append(f"\nğŸ·ï¸ í‚¤ì›Œë“œ: {keywords}")

        # ê°€ê²© ì •ë³´ (ìˆìœ¼ë©´)
        if extracted.get("price"):
            parts.append(f"\nğŸ’° ê²€ìƒ‰ëœ ê°€ê²©: {extracted['price']}")

        if parts:
            return "\n".join(parts)

        return "ì´ë¯¸ì§€ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."


class KakaoLocalAPI:
    """ì¹´ì¹´ì˜¤ ë¡œì»¬ APIë¥¼ í™œìš©í•œ ì‹ë‹¹ ì •ë³´ ê²€ìƒ‰"""

    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv("KAKAO_API_KEY")
        self.base_url = "https://dapi.kakao.com/v2/local/search/keyword.json"

    def search_restaurant(self, query: str) -> Optional[Dict[str, Any]]:
        """
        ì‹ë‹¹ëª…ìœ¼ë¡œ ì¹´ì¹´ì˜¤ ë¡œì»¬ ê²€ìƒ‰
        """
        if not self.api_key:
            return None

        headers = {"Authorization": f"KakaoAK {self.api_key}"}
        params = {
            "query": query,
            "category_group_code": "FD6",  # ìŒì‹ì 
            "size": 5
        }

        try:
            response = requests.get(self.base_url, headers=headers, params=params, timeout=10)
            if response.status_code == 200:
                return response.json()
        except:
            pass
        return None

    def get_place_id_from_url(self, place_url: str) -> Optional[str]:
        """place_urlì—ì„œ place_id ì¶”ì¶œ"""
        match = re.search(r'/(\d+)$', place_url)
        return match.group(1) if match else None

    def get_menu_from_place(self, place_id: str) -> List[Dict[str, str]]:
        """
        ì¹´ì¹´ì˜¤ë§µ placeì—ì„œ ë©”ë‰´ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        ë‚´ë¶€ API ì—”ë“œí¬ì¸íŠ¸ í™œìš©
        """
        menus = []

        # ì¹´ì¹´ì˜¤ë§µ ë‚´ë¶€ API ì—”ë“œí¬ì¸íŠ¸ë“¤ ì‹œë„
        endpoints = [
            f"https://place.map.kakao.com/main/v/{place_id}",
            f"https://place.map.kakao.com/m/main/v/{place_id}",
        ]

        headers = {
            "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X)",
            "Accept": "application/json",
        }

        for endpoint in endpoints:
            try:
                response = requests.get(endpoint, headers=headers, timeout=10)
                if response.status_code == 200:
                    data = response.json()

                    # menuInfo ì°¾ê¸° (JSON êµ¬ì¡°ì—ì„œ)
                    menu_info = self._find_menu_in_json(data)
                    if menu_info:
                        return menu_info
            except:
                continue

        # JSON API ì‹¤íŒ¨ì‹œ HTML í˜ì´ì§€ì—ì„œ ì¶”ì¶œ ì‹œë„
        return self._scrape_menu_from_page(place_id)

    def _find_menu_in_json(self, data: Dict, depth: int = 0) -> List[Dict[str, str]]:
        """JSON ë°ì´í„°ì—ì„œ ë©”ë‰´ ì •ë³´ ì¬ê·€ íƒìƒ‰"""
        if depth > 10:
            return []

        menus = []

        if isinstance(data, dict):
            # menuInfo í‚¤ê°€ ìˆìœ¼ë©´ íŒŒì‹±
            if "menuInfo" in data:
                menu_list = data["menuInfo"]
                if isinstance(menu_list, list):
                    for item in menu_list:
                        if isinstance(item, dict):
                            name = item.get("menu") or item.get("name") or item.get("menuName", "")
                            price = item.get("price") or item.get("menuPrice", "")
                            if name:
                                menus.append({"name": str(name), "price": str(price)})
                return menus

            # menu í‚¤
            if "menu" in data and isinstance(data["menu"], list):
                for item in data["menu"]:
                    if isinstance(item, dict):
                        name = item.get("name") or item.get("menu", "")
                        price = item.get("price", "")
                        if name:
                            menus.append({"name": str(name), "price": str(price)})
                return menus

            # ì¬ê·€ íƒìƒ‰
            for key, value in data.items():
                result = self._find_menu_in_json(value, depth + 1)
                if result:
                    return result

        elif isinstance(data, list):
            for item in data:
                result = self._find_menu_in_json(item, depth + 1)
                if result:
                    return result

        return menus

    def _scrape_menu_from_page(self, place_id: str) -> List[Dict[str, str]]:
        """HTML í˜ì´ì§€ì—ì„œ ë©”ë‰´ ì¶”ì¶œ (í´ë°±)"""
        menus = []

        try:
            url = f"https://place.map.kakao.com/{place_id}"
            headers = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)"}
            response = requests.get(url, headers=headers, timeout=10)

            if response.status_code != 200:
                return menus

            text = response.text

            # JSON ì„ë² ë”© ë°ì´í„°ì—ì„œ ë©”ë‰´ ì°¾ê¸°
            json_patterns = [
                r'"menuInfo"\s*:\s*(\[[^\]]*\])',
                r'"menu"\s*:\s*(\[[^\]]*\])',
            ]

            for pattern in json_patterns:
                match = re.search(pattern, text, re.DOTALL)
                if match:
                    try:
                        import json
                        menu_data = json.loads(match.group(1))
                        for item in menu_data:
                            if isinstance(item, dict):
                                name = item.get("menu") or item.get("name", "")
                                price = item.get("price", "")
                                if name:
                                    menus.append({"name": name, "price": price})
                        if menus:
                            return menus
                    except:
                        pass

        except:
            pass

        return menus

    def search_menu_via_serper(self, query: str) -> str:
        """
        Serper.devë¡œ ì‹ë‹¹/ë©”ë‰´ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ë¹ ë¥¸ ë²„ì „)
        ê²€ìƒ‰ ê²°ê³¼ ìŠ¤ë‹ˆí«ì—ì„œ ë©”ë‰´/ê°€ê²© ì •ë³´ë§Œ ì¶”ì¶œ (ë¸”ë¡œê·¸ í¬ë¡¤ë§ ì œê±°)
        """
        api_key = os.getenv("SERPER_API_KEY") or os.getenv("SERPAPI_KEY")
        if not api_key:
            return ""

        # LLMì´ ìƒì„±í•œ ì¿¼ë¦¬ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        headers = {
            "X-API-KEY": api_key,
            "Content-Type": "application/json"
        }
        data = {
            "q": query,
            "gl": "kr",
            "hl": "ko"
        }

        try:
            response = requests.post("https://google.serper.dev/search", headers=headers, json=data, timeout=10)
            if response.status_code != 200:
                return ""

            result = response.json()
            output = []

            # ê²€ìƒ‰ ê²°ê³¼ ìŠ¤ë‹ˆí«ì—ì„œ ë©”ë‰´/ê°€ê²© ì •ë³´ ì¶”ì¶œ
            for item in result.get("organic", [])[:5]:
                title = item.get("title", "")
                snippet = item.get("snippet", "")

                if snippet:
                    output.append(f"{title}: {snippet}")

            return "\n".join(output)
        except:
            return ""

    def get_menu_via_playwright(self, place_id: str) -> str:
        """
        Playwrightë¡œ ì¹´ì¹´ì˜¤ë§µì—ì„œ ë©”ë‰´ í…ìŠ¤íŠ¸ í¬ë¡¤ë§
        LLMì´ ì§ì ‘ í•´ì„í•˜ë„ë¡ í…ìŠ¤íŠ¸ ë°˜í™˜
        """
        if not PLAYWRIGHT_AVAILABLE:
            return ""

        async def _fetch_menu():
            menu_text = ""
            try:
                async with async_playwright() as p:
                    browser = await p.chromium.launch(
                        headless=True,
                        args=['--no-sandbox', '--disable-dev-shm-usage']
                    )
                    page = await browser.new_page()

                    url = f'https://place.map.kakao.com/{place_id}'
                    await page.goto(url, wait_until='networkidle', timeout=15000)

                    # ë©”ë‰´ íƒ­ í´ë¦­
                    try:
                        menu_tab = await page.query_selector('a[href*="menuInfo"]')
                        if menu_tab:
                            await menu_tab.click()
                            await page.wait_for_timeout(2000)
                    except:
                        pass

                    # ìŠ¤í¬ë¡¤
                    for _ in range(5):
                        await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                        await page.wait_for_timeout(400)

                    # ê°€ê²© ìš”ì†Œì˜ ì¡°ë¶€ëª¨ì—ì„œ ë©”ë‰´ ì¶”ì¶œ
                    price_elements = await page.query_selector_all('//*[contains(text(), "ì›")]')
                    menu_lines = []
                    seen = set()

                    for price_el in price_elements:
                        try:
                            # ì¡°ë¶€ëª¨ ìš”ì†Œ (ë©”ë‰´ëª…+ê°€ê²© í¬í•¨)
                            grandparent = await price_el.evaluate_handle('el => el.parentElement?.parentElement')
                            if grandparent:
                                text = await grandparent.inner_text()
                                text = ' '.join(text.split())
                                # ìœ íš¨í•œ ë©”ë‰´ í•­ëª©ì¸ì§€ í™•ì¸
                                if ('ì›' in text and
                                    len(text) > 5 and
                                    len(text) < 80 and
                                    text not in seen and
                                    'ë¸”ë¡œê·¸' not in text):
                                    seen.add(text)
                                    menu_lines.append(text)
                        except:
                            pass

                    menu_text = '\n'.join(menu_lines[:60])  # ìµœëŒ€ 60ê°œ
                    await browser.close()

            except Exception as e:
                print(f"Playwright í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

            return menu_text

        # ë™ê¸° ì‹¤í–‰
        try:
            return asyncio.run(_fetch_menu())
        except:
            try:
                import nest_asyncio
                nest_asyncio.apply()
                loop = asyncio.get_event_loop()
                return loop.run_until_complete(_fetch_menu())
            except:
                return ""

    def get_reviews_via_playwright(self, place_id: str, max_reviews: int = 15) -> str:
        """
        Playwrightë¡œ ì¹´ì¹´ì˜¤ë§µì—ì„œ í›„ê¸° í¬ë¡¤ë§
        í‰ì , íƒœê·¸ë³„ í‰ê°€, ê°œë³„ í›„ê¸°ë¥¼ êµ¬ì¡°í™”í•˜ì—¬ ë°˜í™˜
        """
        if not PLAYWRIGHT_AVAILABLE:
            return ""

        async def _fetch_reviews():
            result = {
                "rating": None,
                "review_count": 0,
                "tags": {},
                "reviews": []
            }

            try:
                async with async_playwright() as p:
                    browser = await p.chromium.launch(
                        headless=True,
                        args=['--no-sandbox', '--disable-dev-shm-usage']
                    )
                    page = await browser.new_page()

                    url = f'https://place.map.kakao.com/{place_id}'
                    await page.goto(url, wait_until='networkidle', timeout=15000)

                    # í›„ê¸° íƒ­ í´ë¦­
                    all_elements = await page.query_selector_all('a, button, span')
                    tab_clicked = False

                    for el in all_elements:
                        try:
                            text = await el.inner_text()
                            text = text.strip()
                            if 'í›„ê¸°' in text and ('ê°œ' in text or 'ê±´' in text) and len(text) < 30:
                                await el.click()
                                await page.wait_for_timeout(2000)
                                tab_clicked = True
                                break
                        except:
                            continue

                    # í›„ê¸° íƒ­ì´ ì—†ìœ¼ë©´ ë¸”ë¡œê·¸ ë¦¬ë·°ë¡œ í´ë°±
                    is_blog_fallback = False
                    if not tab_clicked:
                        blog_tab = await page.query_selector('a[href*="blog"]')
                        if blog_tab:
                            await blog_tab.click()
                            await page.wait_for_timeout(2000)
                            is_blog_fallback = True
                        else:
                            await browser.close()
                            return "ë§¤ì¥ì£¼ ìš”ì²­ìœ¼ë¡œ í›„ê¸°ê°€ ì œê³µë˜ì§€ ì•ŠëŠ” ì¥ì†Œì…ë‹ˆë‹¤."

                    # ìŠ¤í¬ë¡¤í•˜ì—¬ í›„ê¸° ë¡œë“œ
                    for _ in range(5):
                        await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                        await page.wait_for_timeout(400)

                    # í˜ì´ì§€ í…ìŠ¤íŠ¸ íŒŒì‹±
                    body_text = await page.inner_text('body')
                    lines = [l.strip() for l in body_text.split('\n') if l.strip()]

                    # í‰ì ê³¼ í›„ê¸° ìˆ˜ ì¶”ì¶œ
                    for i, line in enumerate(lines):
                        if line == 'ë³„ì ' and i + 1 < len(lines):
                            try:
                                result["rating"] = float(lines[i + 1])
                            except:
                                pass
                        if 'í›„ê¸°' in line and i + 1 < len(lines):
                            try:
                                count = int(lines[i + 1].replace(',', ''))
                                if count > result["review_count"]:
                                    result["review_count"] = count
                            except:
                                pass

                    # íƒœê·¸ë³„ í‰ê°€ ì¶”ì¶œ (ë§› 245ëª…, ê°€ì„±ë¹„ 137ëª… ë“±)
                    tag_names = ['ë§›', 'ê°€ì„±ë¹„', 'ì¹œì ˆ', 'ë¶„ìœ„ê¸°', 'ì£¼ì°¨', 'ì²­ê²°', 'ì–‘']
                    for i, line in enumerate(lines):
                        if line in tag_names and i + 1 < len(lines):
                            next_line = lines[i + 1]
                            if 'ëª…' in next_line:
                                try:
                                    count = int(next_line.replace('ëª…', '').replace(',', ''))
                                    result["tags"][line] = count
                                except:
                                    pass

                    # ê°œë³„ í›„ê¸° ì¶”ì¶œ
                    reviews = []
                    seen = set()
                    review_keywords = ['ë§›ìˆ', 'ì¢‹', 'ì¶”ì²œ', 'ë˜', 'ìµœê³ ', 'ì•„ì‰¬', 'ë³„ë¡œ', 'ì§œ',
                                      'ì¹œì ˆ', 'ë¶ˆì¹œì ˆ', 'ì›¨ì´íŒ…', 'ê¸°ë‹¤', 'ì–‘ì´', 'ê°€ì„±ë¹„',
                                      'ì¬ë°©ë¬¸', 'ë‹¨ê³¨', 'ì¸ì •', 'ëŒ€ë°•', 'ì‹¤ë§', 'ë§Œì¡±', 'ëƒ„ìƒˆ']

                    for line in lines:
                        if 15 < len(line) < 300 and line not in seen:
                            if line.startswith('http') or 'ì›' in line[:8]:
                                continue
                            if any(skip in line for skip in ['ë”ë³´ê¸°', 'ì ‘ê¸°', 'ì‹ ê³ ', 'ê³µìœ ', 'ì €ì¥', 'ë¡œê·¸ì¸', 'ë°”ë¡œê°€ê¸°']):
                                continue
                            if any(kw in line for kw in review_keywords):
                                seen.add(line)
                                reviews.append(line)
                                if len(reviews) >= max_reviews:
                                    break

                    result["reviews"] = reviews
                    result["is_blog"] = is_blog_fallback
                    await browser.close()

            except Exception as e:
                print(f"í›„ê¸° í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                return f"í›„ê¸° í¬ë¡¤ë§ ì‹¤íŒ¨: {e}"

            # ê²°ê³¼ í¬ë§·íŒ…
            output = []

            if result["rating"]:
                output.append(f"â­ í‰ì : {result['rating']}ì ")
            if result["review_count"]:
                output.append(f"ğŸ“ í›„ê¸°: {result['review_count']}ê°œ")

            if result["tags"]:
                output.append("")
                output.append("[íƒœê·¸ë³„ í‰ê°€]")
                for tag, count in sorted(result["tags"].items(), key=lambda x: -x[1]):
                    output.append(f"  â€¢ {tag}: {count}ëª…")

            if result["reviews"]:
                output.append("")
                output.append(f"[ìµœê·¼ í›„ê¸° {len(result['reviews'])}ê°œ]")
                for r in result["reviews"]:
                    output.append(f"  â€¢ {r}")

            return '\n'.join(output) if output else "í›„ê¸°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # ë™ê¸° ì‹¤í–‰
        try:
            return asyncio.run(_fetch_reviews())
        except:
            try:
                import nest_asyncio
                nest_asyncio.apply()
                loop = asyncio.get_event_loop()
                return loop.run_until_complete(_fetch_reviews())
            except:
                return ""

    def _crawl_blog_menu(self, blog_url: str) -> str:
        """
        ë¸”ë¡œê·¸ì—ì„œ ë©”ë‰´/ê°€ê²© ì •ë³´ í¬ë¡¤ë§
        ë„¤ì´ë²„ ë¸”ë¡œê·¸, í‹°ìŠ¤í† ë¦¬ ì§€ì› (í´ë°±ìš©)
        """
        try:
            # ë„¤ì´ë²„ ë¸”ë¡œê·¸ ëª¨ë°”ì¼ ë²„ì „ìœ¼ë¡œ ë³€í™˜
            if "blog.naver.com" in blog_url and "m.blog" not in blog_url:
                blog_url = blog_url.replace("blog.naver.com", "m.blog.naver.com")

            headers = {
                "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X)",
                "Accept-Encoding": "gzip, deflate",
            }

            resp = requests.get(blog_url, headers=headers, timeout=10)
            if resp.status_code != 200:
                return ""

            text = resp.text

            # ì—¬ëŸ¬ ê°€ê²© íŒ¨í„´ ì‹œë„
            patterns = [
                r'([ê°€-í£]+(?:\s*[ê°€-í£]+)*)\s*[:\-\s]*([\d,]+)\s*ì›',  # ë©”ë‰´: ê°€ê²©ì›
                r'([ê°€-í£]{2,12})\s+([\d]{1,2}[,.]?\d{3})\s*ì›?',  # ë©”ë‰´ ê°€ê²©
                r'([ê°€-í£]{2,12}ê°ìíŠ€ê¹€|[ê°€-í£]{2,12}ë³¶ìŒë°¥|[ê°€-í£]{2,12}í† í•‘)\s*([\d,]+)',  # íŠ¹ì • ë©”ë‰´ íŒ¨í„´
            ]

            menu_items = []
            seen = set()
            exclude = ['í‰ê· ', 'ì´ìš©', 'ìµœì†Œ', 'ë°°ë‹¬', 'ì£¼ë¬¸', 'ê²°ì œ', 'í•©ê³„', 'ì´', 'í• ì¸', 'ì¸ë¶„', 'ê°€ê²©', 'ì›ê°€']

            for pattern in patterns:
                prices = re.findall(pattern, text)
                for name, price in prices:
                    name = name.strip()
                    # ê°€ê²© ì •ë¦¬ (ì½¤ë§ˆ, ì  ì²˜ë¦¬)
                    price = price.replace(',', '').replace('.', '')
                    if (len(name) >= 2 and
                        name not in seen and
                        not any(ex in name for ex in exclude) and
                        price.isdigit() and int(price) >= 1000):  # 1000ì› ì´ìƒë§Œ
                        seen.add(name)
                        price_formatted = f"{int(price):,}"
                        menu_items.append(f"  - {name}: {price_formatted}ì›")

            return "\n".join(menu_items[:15]) if menu_items else ""
        except:
            return ""

    def get_place_detail(self, place_url: str) -> Dict[str, Any]:
        """
        ì¹´ì¹´ì˜¤ë§µ place_urlì—ì„œ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§
        """
        info = {
            "name": None,
            "address": None,
            "phone": None,
            "menus": [],
            "hours": None
        }

        try:
            headers = {"User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X)"}
            response = requests.get(place_url, headers=headers, timeout=10)

            if response.status_code != 200:
                return info

            text = response.text

            # ë©”ë‰´ ì •ë³´ ì¶”ì¶œ (JSON-LD ë˜ëŠ” HTMLì—ì„œ)
            # ì¹´ì¹´ì˜¤ë§µ í˜ì´ì§€ êµ¬ì¡°ì— ë§ê²Œ íŒŒì‹±
            menu_pattern = r'"menuInfo":\s*\[(.*?)\]'
            menu_match = re.search(menu_pattern, text, re.DOTALL)
            if menu_match:
                menu_text = menu_match.group(1)
                # ë©”ë‰´ëª…ê³¼ ê°€ê²© ì¶”ì¶œ
                items = re.findall(r'"menu":\s*"([^"]+)".*?"price":\s*"([^"]*)"', menu_text)
                for name, price in items:
                    info["menus"].append({"name": name, "price": price})

            # ë©”ë‰´ ì •ë³´ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
            if not info["menus"]:
                # HTML íƒœê·¸ ì œê±°
                clean_text = re.sub(r'<[^>]+>', ' ', text)
                clean_text = ' '.join(clean_text.split())

                # ê°€ê²© íŒ¨í„´ìœ¼ë¡œ ë©”ë‰´ ì°¾ê¸°
                price_menus = re.findall(r'([ê°€-í£]{2,15})\s*([\d,]+)\s*ì›', clean_text)
                for name, price in price_menus[:10]:
                    if name not in ['ë©”ë‰´', 'ê°€ê²©', 'ì˜ì—…', 'ì •ë³´']:
                        info["menus"].append({"name": name, "price": f"{price}ì›"})

        except Exception as e:
            pass

        return info

    def extract_restaurant_name(self, titles: List[str]) -> Optional[str]:
        """
        ê²€ìƒ‰ ê²°ê³¼ ì œëª©ë“¤ì—ì„œ ì‹ë‹¹ëª… ì¶”ì¶œ
        - ì—¬ëŸ¬ ì œëª©ì—ì„œ ê³µí†µìœ¼ë¡œ ë‚˜ì˜¤ëŠ” ì´ë¦„ ìš°ì„ 
        """
        all_text = " ".join(titles)
        candidates = []

        # 1. ìŒì‹ ì¢…ë¥˜ê°€ í¬í•¨ëœ ì‹ë‹¹ëª… (ê°€ì¥ ì‹ ë¢°ë„ ë†’ìŒ)
        food_keywords = ['ìˆœëŒ€', 'êµ­ë°¥', 'ìš°ë™', 'êµì', 'ë–¡ë³¶ì´', 'ì¹˜í‚¨', 'í”¼ì', 'ì¹¼êµ­ìˆ˜', 'ëƒ‰ë©´', 'ì„¤ë íƒ•', 'ê³°íƒ•', 'ì‚¼ê²¹ì‚´', 'ê°ˆë¹„']
        for kw in food_keywords:
            # "XXXìˆœëŒ€", "XXXêµ­ë°¥" ë“± íŒ¨í„´
            pattern = rf'([ê°€-í£]{{2,10}}{kw}[ê°€-í£]{{0,5}})'
            matches = re.findall(pattern, all_text)
            for m in matches:
                # ì§€ì ëª…ë§Œ ìˆëŠ” ê²½ìš° ì œì™¸
                if not re.match(r'^(ë³¸ì |ì§ì˜ì |.+ì )$', m):
                    candidates.append(m)

        # 2. ì œëª© ì‹œì‘ ë¶€ë¶„ (ëŒ€ê´„í˜¸ ë’¤)
        for title in titles:
            bracket_match = re.search(r'\]\s*([ê°€-í£a-zA-Z0-9]{3,15})', title)
            if bracket_match:
                name = bracket_match.group(1)
                if not re.match(r'^(ë³¸ì |ì§ì˜ì |.+ì )$', name):
                    candidates.append(name)

        # 3. "ì‹ë‹¹ëª… + ì§€ì " íŒ¨í„´ì—ì„œ ì‹ë‹¹ëª…ë§Œ ì¶”ì¶œ
        branch_pattern = r'([ê°€-í£]{2,10})\s*(?:ë³¸ì |ì§ì˜ì |[ê°€-í£]+ì )'
        branch_matches = re.findall(branch_pattern, all_text)
        candidates.extend(branch_matches)

        # ì¼ë°˜ ë‹¨ì–´ ì œì™¸
        exclude = ['ë§›ì§‘', 'í›„ê¸°', 'ë¦¬ë·°', 'ë°©ë¬¸', 'ì¶”ì²œ', 'ì›¨ì´íŒ…', 'ìš°ë¦¬ì§‘', 'ì‹œì²­ì—­',
                   'ì„œìš¸', 'ë¶€ì‚°', 'ëŒ€ì „', 'ì¸ì²œ', 'ì‚¼ì„±ë™', 'ê°•ë‚¨ì—­', 'ë©”ë‰´', 'ê°€ê²©',
                   'ìƒë°©ì†¡', 'ì˜¤ëŠ˜', 'ì €ë…', 'ëª…ê°€', 'ìœ ëª…', 'ë°”ì‚­í•˜ë‹ˆ', 'ë§›ìˆê² ë‹¤',
                   'ë³¸ì ', 'ì§ì˜ì ', 'ì‹œì²­ì§ì˜ì ']

        filtered = [c for c in candidates if c not in exclude and len(c) >= 3]

        if not filtered:
            return None

        # ë¹ˆë„ìˆ˜ ê³„ì‚°
        counter = Counter(filtered)

        # ì—¬ëŸ¬ ë²ˆ ë“±ì¥í•˜ëŠ” ê²ƒ ìš°ì„ 
        for name, count in counter.most_common(10):
            if count >= 2:
                return name

        # ìŒì‹ í‚¤ì›Œë“œ í¬í•¨ëœ ê²ƒ ìš°ì„ 
        for name, count in counter.most_common(5):
            if any(kw in name for kw in food_keywords):
                # ì¡°ì‚¬ ì œê±°
                name = re.sub(r'[ì„ë¥¼ì´ê°€ì˜ì—ì„œ]$', '', name)
                return name

        result = counter.most_common(1)[0][0]
        # ì¡°ì‚¬ ì œê±°
        result = re.sub(r'[ì„ë¥¼ì´ê°€ì˜ì—ì„œ]$', '', result)
        return result


# ì „ì—­ ê²€ìƒ‰ê¸° ì¸ìŠ¤í„´ìŠ¤
_searcher: Optional[SerperImageSearcher] = None
_kakao: Optional[KakaoLocalAPI] = None


def get_searcher() -> SerperImageSearcher:
    """ê²€ìƒ‰ê¸° ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _searcher
    if _searcher is None:
        _searcher = SerperImageSearcher()
    return _searcher


def get_kakao() -> KakaoLocalAPI:
    """ì¹´ì¹´ì˜¤ API ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _kakao
    if _kakao is None:
        _kakao = KakaoLocalAPI()
    return _kakao


def extract_blog_content(url: str) -> Dict[str, Any]:
    """
    ë¸”ë¡œê·¸ í˜ì´ì§€ì—ì„œ ìŒì‹ ê´€ë ¨ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    íŠ¹ì • ë©”ë‰´ëª…ì„ í•˜ë“œì½”ë”©í•˜ì§€ ì•Šê³  ë²”ìš©ì ìœ¼ë¡œ ì¶”ì¶œ
    """
    result = {"url": url, "content": ""}

    try:
        # ë„¤ì´ë²„ ë¸”ë¡œê·¸ ëª¨ë°”ì¼ ë²„ì „ìœ¼ë¡œ ë³€í™˜
        if 'blog.naver.com' in url and 'm.blog' not in url:
            url = url.replace('blog.naver.com', 'm.blog.naver.com')

        headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X)'
        }
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code != 200:
            return result

        text = response.text

        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<script[^>]*>.*?</script>', '', text, flags=re.DOTALL)
        text = re.sub(r'<style[^>]*>.*?</style>', '', text, flags=re.DOTALL)
        text = re.sub(r'<[^>]+>', ' ', text)
        text = ' '.join(text.split())

        # ìŒì‹/ì£¼ë¬¸ ê´€ë ¨ ë¬¸ì¥ ì¶”ì¶œ (ë²”ìš© í‚¤ì›Œë“œ)
        food_keywords = ['ì£¼ë¬¸', 'ì‹œì¼°', 'ë¨¹ì—ˆ', 'ë©”ë‰´', 'ë§›ìˆ', 'ë°”ì‚­', 'ì«„ê¹ƒ', 'í† í•‘', 'ì†ŒìŠ¤', 'ê°€ê²©', 'ì›']
        sentences = re.split(r'[.!?ã€‚]', text)

        relevant_sentences = []
        for sentence in sentences:
            if any(kw in sentence for kw in food_keywords):
                if 20 < len(sentence) < 200:  # ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ë¬¸ì¥ ì œì™¸
                    relevant_sentences.append(sentence.strip())

        result["content"] = ' '.join(relevant_sentences[:10])  # ìµœëŒ€ 10ë¬¸ì¥

    except Exception as e:
        pass

    return result


@tool
def search_food_by_image(image_source: str) -> str:
    """
    ìƒˆë¡œìš´ ìŒì‹ ì´ë¯¸ì§€ê°€ ìˆì„ ë•Œë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
    ì´ë¯¸ì§€ URL ë˜ëŠ” ë¡œì»¬ íŒŒì¼ ê²½ë¡œë¥¼ ë°›ì•„ Google Lensë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.

    ì£¼ì˜: ì´ì „ ëŒ€í™”ì—ì„œ ì´ë¯¸ ì¸ì‹í•œ ìŒì‹ì— ëŒ€í•´ì„œëŠ” ì´ ë„êµ¬ë¥¼ ë‹¤ì‹œ í˜¸ì¶œí•˜ì§€ ë§ˆì„¸ìš”.
    í›„ì† ì§ˆë¬¸(ì‹ë‹¹ ì •ë³´, ë©”ë‰´ ê°€ê²© ë“±)ì€ ë‹¤ë¥¸ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

    Args:
        image_source: ì´ë¯¸ì§€ URL ë˜ëŠ” ë¡œì»¬ íŒŒì¼ ê²½ë¡œ (í•„ìˆ˜)

    Returns:
        Google ì´ë¯¸ì§€ ê²€ìƒ‰ ê²°ê³¼ + ë¸”ë¡œê·¸ ë³¸ë¬¸
    """
    # ì´ë¯¸ì§€ ê²½ë¡œ ìœ íš¨ì„± ê²€ì‚¬
    if not image_source or not image_source.strip():
        return "[ì´ë¯¸ì§€ ì—†ìŒ] ì´ ë„êµ¬ëŠ” ìƒˆ ì´ë¯¸ì§€ê°€ ìˆì„ ë•Œë§Œ ì‚¬ìš©í•˜ì„¸ìš”. ì´ì „ ëŒ€í™”ì—ì„œ íŒŒì•…í•œ ì •ë³´ë¥¼ í™œìš©í•´ì£¼ì„¸ìš”."

    image_source = image_source.strip()

    # URLì´ ì•„ë‹ˆê³  íŒŒì¼ë„ ì•„ë‹Œ ê²½ìš° (í…ìŠ¤íŠ¸ë§Œ ì „ë‹¬ëœ ê²½ìš°)
    if not image_source.startswith(('http://', 'https://', '/')):
        return "[ì´ë¯¸ì§€ ì—†ìŒ] ìœ íš¨í•œ ì´ë¯¸ì§€ ê²½ë¡œê°€ ì•„ë‹™ë‹ˆë‹¤. ì´ì „ ëŒ€í™”ì—ì„œ íŒŒì•…í•œ ì •ë³´ë¥¼ í™œìš©í•´ì£¼ì„¸ìš”."

    # ë¡œì»¬ íŒŒì¼ì¸ ê²½ìš° ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if not image_source.startswith(('http://', 'https://')) and not os.path.exists(image_source):
        return f"[ì´ë¯¸ì§€ ì—†ìŒ] íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_source}. ì´ì „ ëŒ€í™”ì—ì„œ íŒŒì•…í•œ ì •ë³´ë¥¼ í™œìš©í•´ì£¼ì„¸ìš”."

    searcher = get_searcher()

    image_url = searcher.get_image_url(image_source)
    if not image_url:
        return f"ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_source}"

    result = searcher.search_with_combined(image_url)

    if "error" in result:
        return f"ê²€ìƒ‰ ì‹¤íŒ¨: {result['error']}"

    output = []
    blog_links = []

    # ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ 10ê°œ) - title + link + thumbnail
    visual = result.get("visual_matches", [])
    thumbnails = []  # ì¸ë„¤ì¼ URL ìˆ˜ì§‘

    if visual:
        output.append("[ê²€ìƒ‰ ê²°ê³¼]")
        for i, v in enumerate(visual[:10], 1):
            title = v.get("title", "")
            snippet = v.get("snippet", "")
            link = v.get("link", "")
            thumbnail = v.get("thumbnail", "")

            if title:
                line = f"{i}. {title}"
                if snippet:
                    line += f" - {snippet[:100]}"
                output.append(line)

            # ì¸ë„¤ì¼ ìˆ˜ì§‘ (ìƒìœ„ 3ê°œ)
            if thumbnail and len(thumbnails) < 3:
                thumbnails.append(thumbnail)

            # ë¸”ë¡œê·¸ ë§í¬ ìˆ˜ì§‘
            if link and ('blog.naver.com' in link or 'tistory.com' in link):
                blog_links.append(link)

    # ì¸ë„¤ì¼ ì´ë¯¸ì§€ URL ì¶”ê°€
    if thumbnails:
        output.append("\n[ê²€ìƒ‰ ê²°ê³¼ ì´ë¯¸ì§€]")
        for url in thumbnails:
            output.append(f"[IMAGE:{url}]")

    # ë¸”ë¡œê·¸ ë³¸ë¬¸ ì¶”ì¶œ (ìƒìœ„ 3ê°œ)
    if blog_links:
        output.append("\n[ë¸”ë¡œê·¸ ë³¸ë¬¸ (ë©”ë‰´ íŒë‹¨ ì°¸ê³ ìš©)]")
        for i, link in enumerate(blog_links[:3], 1):
            blog_data = extract_blog_content(link)
            if blog_data["content"]:
                output.append(f"\n--- ë¸”ë¡œê·¸ {i} ---")
                output.append(blog_data["content"][:1000])  # ìµœëŒ€ 1000ì

    # ì´ë¯¸ì§€ ë‚´ í…ìŠ¤íŠ¸
    texts = result.get("text", [])
    if texts:
        text_list = [t.get("text", "") for t in texts[:5] if t.get("text")]
        if text_list:
            output.append(f"\n[ì´ë¯¸ì§€ í…ìŠ¤íŠ¸] {', '.join(text_list)}")

    output.append("\n[íŒë‹¨ ìš”ì²­]")
    output.append("1. ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ ê²°ê³¼ ì œëª©, ë¸”ë¡œê·¸ ë³¸ë¬¸ì„ ì°¸ê³ í•˜ì„¸ìš”.")
    output.append("2. ìŒì‹ ì´ë¦„ë§Œ ë¬¼ì–´ë³´ë©´: '~ë¡œ ë³´ì…ë‹ˆë‹¤' + ì‹ë‹¹ì´ ë³´ì´ë©´ 'í˜¹ì‹œ OOì—ì„œ ë“œì…¨ë‚˜ìš”?'")
    output.append("3. ì‹ë‹¹/ë©”ë‰´ëª…ê¹Œì§€ ë¬¼ì–´ë³´ë©´: ê°€ëŠ¥ì„± ìˆëŠ” ì‹ë‹¹ 2~3ê³³ì„ í›„ë³´ë¡œ ë‚˜ì—´í•˜ì„¸ìš”.")
    output.append("4. í•˜ë‚˜ë¡œ ë‹¨ì •ì§“ì§€ ë§ê³  '~ì¼ ìˆ˜ë„ ìˆê³ , ~ì¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤' í˜•íƒœë¡œ ë‹µë³€í•˜ì„¸ìš”.")

    return "\n".join(output) if output else "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"


@tool
def search_restaurant_info(query: str) -> str:
    """
    ì‹ë‹¹ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤. ì‹ë‹¹ëª…, ì§€ì—­+ìŒì‹, ì§€ì—­+ë§›ì§‘ ë“± ë‹¤ì–‘í•œ ê²€ìƒ‰ì–´ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

    Args:
        query: ê²€ìƒ‰ì–´ (ì‹ë‹¹ëª…, ì§€ì—­+ìŒì‹, ì§€ì—­+ë§›ì§‘ ë“±)

    Returns:
        ì‹ë‹¹ ì •ë³´ (ì´ë¦„, ì£¼ì†Œ, ì „í™”ë²ˆí˜¸, ì¹´í…Œê³ ë¦¬, ë©”ë‰´)
    """
    kakao = get_kakao()

    result = kakao.search_restaurant(query)

    output = []
    place_id = None

    # ì¹´ì¹´ì˜¤ë§µ ê²€ìƒ‰ ê²°ê³¼
    if result and result.get("documents"):
        first_place = result["documents"][0]
        place_url = first_place.get("place_url", "")
        place_id = kakao.get_place_id_from_url(place_url) if place_url else None

        # ì—¬ëŸ¬ ì‹ë‹¹ ì¢Œí‘œ ìˆ˜ì§‘
        coords_list = []

        for i, place in enumerate(result["documents"][:3], 1):
            output.append(f"[{i}] {place.get('place_name', '')}")
            output.append(f"   ì£¼ì†Œ: {place.get('road_address_name', '') or place.get('address_name', '')}")
            output.append(f"   ì „í™”: {place.get('phone', '')}")
            output.append(f"   ì¹´í…Œê³ ë¦¬: {place.get('category_name', '')}")
            # ì¹´ì¹´ì˜¤ë§µ ë§í¬ ì¶”ê°€
            p_url = place.get('place_url', '')
            if p_url:
                output.append(f"   ğŸ—ºï¸ ì§€ë„: {p_url}")
            output.append("")

            # ì¢Œí‘œ ë° ì •ë³´ ìˆ˜ì§‘
            x = place.get('x', '')  # longitude
            y = place.get('y', '')  # latitude
            name = place.get('place_name', '')
            address = place.get('road_address_name', '') or place.get('address_name', '')
            phone = place.get('phone', '')
            category = place.get('category_name', '').split(' > ')[-1] if place.get('category_name') else ''
            place_url = place.get('place_url', '')
            if x and y:
                # | ë¡œ í•„ë“œ êµ¬ë¶„ (ì´ë¦„|ì£¼ì†Œ|ì „í™”|ì¹´í…Œê³ ë¦¬|ì¹´ì¹´ì˜¤ë§µURL)
                info = f"{name}|{address}|{phone}|{category}|{place_url}"
                coords_list.append(f"{y},{x},{info}")

        # ì—¬ëŸ¬ ì‹ë‹¹ ì¢Œí‘œë¥¼ MAP íƒœê·¸ì— í¬í•¨ (ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ êµ¬ë¶„)
        if coords_list:
            coords_str = ";".join(coords_list)
            output.insert(0, f"[MAP:{coords_str}]")

    # Playwrightë¡œ ë©”ë‰´ í…ìŠ¤íŠ¸ í¬ë¡¤ë§ (LLMì´ í•´ì„)
    menu_text = ""
    if place_id and PLAYWRIGHT_AVAILABLE:
        menu_text = kakao.get_menu_via_playwright(place_id)

    if menu_text:
        output.append("[ë©”ë‰´íŒ]")
        output.append(menu_text)
    else:
        # í´ë°±: Serper ê²€ìƒ‰
        menu_info = kakao.search_menu_via_serper(query)
        if menu_info:
            output.append("[ë©”ë‰´ ê²€ìƒ‰ ê²°ê³¼]")
            output.append(menu_info)

    if not output:
        return f"'{query}' ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"

    return "\n".join(output)


@tool
def search_recipe_online(query: str) -> str:
    """
    ì¸í„°ë„·ì—ì„œ ë ˆì‹œí”¼ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤. LLMì´ ì‚¬ìš©ì ì§ˆë¬¸ì— ë§ê²Œ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬ (ì˜ˆ: "ê¹€ì¹˜ì°Œê°œ ë ˆì‹œí”¼", "ë°±ì¢…ì› ëœì¥ì°Œê°œ", "ì´ˆê°„ë‹¨ ê³„ë€ì°œ ë§Œë“œëŠ”ë²•")

    Returns:
        ë ˆì‹œí”¼ ì •ë³´ (ì¬ë£Œ, ì¡°ë¦¬ ìˆœì„œ) - ìµœëŒ€ 3ê°œ ë ˆì‹œí”¼
    """
    searcher = get_searcher()

    # LLMì´ ìƒì„±í•œ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
    search_result = searcher.search_text(query)

    if "error" in search_result:
        return f"ê²€ìƒ‰ ì‹¤íŒ¨: {search_result['error']}"

    organic = search_result.get("organic_results", [])

    if not organic:
        return f"'{query}' ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

    # ìƒìœ„ 3ê°œ ê²°ê³¼ í¬ë¡¤ë§
    output = [f"[ê²€ìƒ‰: {query}]"]
    for i, item in enumerate(organic[:3], 1):
        link = item.get("link", "")
        recipe_data = _crawl_recipe_fast(link)
        output.append(f"\n=== ë ˆì‹œí”¼ {i} ===\n{recipe_data}")

    return "\n".join(output)


def _crawl_recipe_fast(url: str) -> str:
    """requestsë¡œ ë¹ ë¥¸ ë ˆì‹œí”¼ í¬ë¡¤ë§ (Playwright ì—†ì´)"""
    if not BS4_AVAILABLE:
        return f"BeautifulSoup ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. pip install beautifulsoup4"

    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        resp = requests.get(url, headers=headers, timeout=10)
        resp.encoding = 'utf-8'

        if resp.status_code != 200:
            return f"í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {url}"

        soup = BeautifulSoup(resp.text, 'html.parser')

        # ë§Œê°œì˜ë ˆì‹œí”¼ - í•„ìš”í•œ ì •ë³´ë§Œ ì„ íƒì  ì¶”ì¶œ
        if '10000recipe.com' in url:
            output = []

            # ì œëª©
            title_el = soup.select_one('.view2_summary h3, .view2_summary_tit')
            if title_el:
                output.append(f"[{title_el.get_text(strip=True)}]")

            output.append(f"ì¶œì²˜: {url}")

            # ì„¤ëª…
            desc_el = soup.select_one('.view2_summary_in')
            if desc_el:
                output.append(f"\n{desc_el.get_text(strip=True)}")

            # ì¸ë¶„/ì‹œê°„/ë‚œì´ë„
            info_els = soup.select('.view2_summary_info span')
            if info_els:
                info_text = ' | '.join([el.get_text(strip=True) for el in info_els])
                output.append(f"({info_text})")

            # ì¬ë£Œ
            ingredients = []
            for li in soup.select('.ready_ingre3 li'):
                text = li.get_text(strip=True).replace('êµ¬ë§¤', '').strip()
                if text:
                    ingredients.append(text)
            if ingredients:
                output.append("\n[ì¬ë£Œ]")
                for ing in ingredients[:20]:
                    output.append(f"  - {ing}")

            # ì¡°ë¦¬ ìˆœì„œ
            steps = []
            for step in soup.select('.view_step_cont'):
                text = step.get_text(strip=True)
                if text:
                    steps.append(text)
            if steps:
                output.append("\n[ì¡°ë¦¬ ìˆœì„œ]")
                for i, step in enumerate(steps[:15], 1):
                    if len(step) > 200:
                        step = step[:200] + "..."
                    output.append(f"  {i}. {step}")

            # AI ë¦¬ë·° ìš”ì•½
            ai_ratio = soup.select_one('.reply_ai_t2')
            ai_summary = soup.select_one('.reply_ai_sum')
            if ai_ratio or ai_summary:
                output.append("\n[AI ë¦¬ë·° ìš”ì•½]")
                if ai_ratio:
                    output.append(f"  {ai_ratio.get_text(strip=True)}")
                if ai_summary:
                    output.append(f"  {ai_summary.get_text(strip=True)[:300]}")

            # í›„ê¸° 5ê°œ
            reviews = soup.select('.reply_list')[:5]
            if reviews:
                output.append("\n[í›„ê¸°]")
                for r in reviews:
                    text = r.get_text(strip=True)[:150]
                    if text:
                        output.append(f"  - {text}")

            return "\n".join(output)

        # ë„¤ì´ë²„ ë¸”ë¡œê·¸ / í‹°ìŠ¤í† ë¦¬ / ê¸°íƒ€ - ë³¸ë¬¸ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ì¶”ì¶œ
        else:
            # ë„¤ì´ë²„ ë¸”ë¡œê·¸ëŠ” ëª¨ë°”ì¼ ë²„ì „ ì‚¬ìš©
            if 'blog.naver.com' in url:
                url = url.replace('blog.naver.com', 'm.blog.naver.com')
                resp = requests.get(url, headers=headers, timeout=10)
                soup = BeautifulSoup(resp.text, 'html.parser')
                content = soup.select_one('.se-main-container, #postViewArea, .post-view')
            else:
                content = soup.select_one('article, .post-content, .entry-content, main, .content')
                if not content:
                    content = soup.body

            if content:
                text = content.get_text(separator='\n')
                lines = [l.strip() for l in text.split('\n') if l.strip()]
                # ë³¸ë¬¸ í…ìŠ¤íŠ¸ (ìµœëŒ€ 3500ì)
                body_text = '\n'.join(lines)[:3500]
                return f"[ë ˆì‹œí”¼]\nì¶œì²˜: {url}\n\n{body_text}"

    except Exception as e:
        return f"í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}\nURL: {url}"

    return f"ë ˆì‹œí”¼ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\nURL: {url}"


@tool
def get_restaurant_reviews(restaurant_name: str) -> str:
    """
    ì‹ë‹¹ì˜ í›„ê¸°ë¥¼ ì¹´ì¹´ì˜¤ë§µì—ì„œ ê°€ì ¸ì™€ ìš”ì•½í•©ë‹ˆë‹¤.
    ì‚¬ìš©ìê°€ "í›„ê¸° ì–´ë•Œ", "ë¦¬ë·° ì•Œë ¤ì¤˜", "í‰ê°€ ì–´ë•Œ" ë“±ì„ ë¬¼ì–´ë³¼ ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

    Args:
        restaurant_name: ì‹ë‹¹ ì´ë¦„ (ì˜ˆ: "ìš”ë¯¸ìš°ëˆêµì ê°•ë‚¨ì ")

    Returns:
        ì‹ë‹¹ í›„ê¸° ëª©ë¡ ë° ìš”ì•½
    """
    if not PLAYWRIGHT_AVAILABLE:
        return "Playwrightê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ í›„ê¸°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    kakao = get_kakao()

    # 1. ì‹ë‹¹ ê²€ìƒ‰
    result = kakao.search_restaurant(restaurant_name)

    if not result or not result.get("documents"):
        return f"'{restaurant_name}' ì‹ë‹¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # ì²« ë²ˆì§¸ ê²°ê³¼ ì‚¬ìš©
    place = result["documents"][0]
    place_name = place.get("place_name", "")
    place_url = place.get("place_url", "")
    address = place.get("address_name", "")

    # 2. place_id ì¶”ì¶œ
    place_id = kakao.get_place_id_from_url(place_url)

    if not place_id:
        return f"'{restaurant_name}' í›„ê¸° í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # 3. í›„ê¸° í¬ë¡¤ë§
    reviews_text = kakao.get_reviews_via_playwright(place_id, max_reviews=15)

    # 4. ê²°ê³¼ í¬ë§·íŒ…
    output = []
    output.append(f"[{place_name} í›„ê¸°]")
    output.append(f"ğŸ“ ì£¼ì†Œ: {address}")
    output.append(f"ğŸ”— ì¹´ì¹´ì˜¤ë§µ: {place_url}")
    output.append("")

    if reviews_text:
        output.append("ğŸ“ ë°©ë¬¸ì í›„ê¸°:")
        output.append(reviews_text)
        output.append("")
        output.append("[ìš”ì•½ ìš”ì²­] ìœ„ í›„ê¸°ë“¤ì„ ë¶„ì„í•´ì„œ ì¥ì , ë‹¨ì , ì¶”ì²œ ë©”ë‰´ ë“±ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.")
    else:
        output.append("í›„ê¸°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì•„ì§ ë“±ë¡ëœ í›„ê¸°ê°€ ì—†ê±°ë‚˜ í¬ë¡¤ë§ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

    return "\n".join(output)


@tool
def get_nutrition_info(query: str) -> str:
    """
    ìŒì‹ì˜ ì˜ì–‘ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤. LLMì´ ì‚¬ìš©ì ì§ˆë¬¸ì— ë§ê²Œ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬ (ì˜ˆ: "ê¹€ì¹˜ì°Œê°œ ì¹¼ë¡œë¦¬", "ë§˜ìŠ¤í„°ì¹˜ ì‹¸ì´ë²„ê±° ë‹¨ë°±ì§ˆ", "ìŠ¤íƒ€ë²…ìŠ¤ ì•„ë©”ë¦¬ì¹´ë…¸ 1ì” ì—´ëŸ‰")

    Returns:
        ì˜ì–‘ì •ë³´ ê²€ìƒ‰ ê²°ê³¼ (ë³¸ë¬¸ í¬ë¡¤ë§ í¬í•¨)
    """
    searcher = get_searcher()

    # LLMì´ ìƒì„±í•œ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
    search_result = searcher.search_text(query)

    if "error" in search_result:
        return f"ê²€ìƒ‰ ì‹¤íŒ¨: {search_result['error']}"

    organic = search_result.get("organic_results", [])

    if not organic:
        return f"'{query}' ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

    output = [f"[ê²€ìƒ‰: {query}]"]

    # ìƒìœ„ 3ê°œ í˜ì´ì§€ ë³¸ë¬¸ í¬ë¡¤ë§
    for item in organic[:3]:
        title = item.get("title", "")
        link = item.get("link", "")

        content = _crawl_nutrition_page(link)

        if content:
            output.append(f"\n=== {title} ===")
            output.append(f"ì¶œì²˜: {link}")
            output.append(content)

    return "\n".join(output)


def _crawl_nutrition_page(url: str) -> str:
    """ì˜ì–‘ì •ë³´ í˜ì´ì§€ ë³¸ë¬¸ í¬ë¡¤ë§"""
    if not BS4_AVAILABLE:
        return ""

    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        # ë„¤ì´ë²„ ë¸”ë¡œê·¸ ëª¨ë°”ì¼ ë³€í™˜
        if 'blog.naver.com' in url and 'm.blog' not in url:
            url = url.replace('blog.naver.com', 'm.blog.naver.com')

        resp = requests.get(url, headers=headers, timeout=10)
        resp.encoding = 'utf-8'

        if resp.status_code != 200:
            return ""

        soup = BeautifulSoup(resp.text, 'html.parser')

        # script/styleë§Œ ì œê±°
        for tag in soup(['script', 'style']):
            tag.decompose()

        if soup.body:
            text = soup.body.get_text(separator='\n')
            lines = [l.strip() for l in text.split('\n') if l.strip()]
            return '\n'.join(lines)[:2000]

    except Exception:
        return ""

    return ""


