"""ì´ë¯¸ì§€ ê²€ìƒ‰ ë„êµ¬"""

import os
import re
from typing import Dict, Any
from langchain_core.tools import tool
from langgraph.config import get_stream_writer

try:
    import requests
except ImportError:
    pass

from ..services import get_searcher


def extract_blog_content(url: str) -> Dict[str, Any]:
    """ë¸”ë¡œê·¸ í˜ì´ì§€ì—ì„œ ìŒì‹ ê´€ë ¨ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    result = {"url": url, "content": ""}

    try:
        if 'blog.naver.com' in url and 'm.blog' not in url:
            url = url.replace('blog.naver.com', 'm.blog.naver.com')

        headers = {'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X)'}
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code != 200:
            return result

        text = response.text
        text = re.sub(r'<script[^>]*>.*?</script>', '', text, flags=re.DOTALL)
        text = re.sub(r'<style[^>]*>.*?</style>', '', text, flags=re.DOTALL)
        text = re.sub(r'<[^>]+>', ' ', text)
        text = ' '.join(text.split())

        food_keywords = ['ì£¼ë¬¸', 'ì‹œì¼°', 'ë¨¹ì—ˆ', 'ë©”ë‰´', 'ë§›ìˆ', 'ë°”ì‚­', 'ì«„ê¹ƒ', 'í† í•‘', 'ì†ŒìŠ¤', 'ê°€ê²©', 'ì›']
        sentences = re.split(r'[.!?ã€‚]', text)

        relevant_sentences = []
        for sentence in sentences:
            if any(kw in sentence for kw in food_keywords):
                if 20 < len(sentence) < 200:
                    relevant_sentences.append(sentence.strip())

        result["content"] = ' '.join(relevant_sentences[:10])
    except:
        pass

    return result


@tool
def search_food_by_image(image_source: str) -> str:
    """
    ìƒˆë¡œìš´ ìŒì‹ ì´ë¯¸ì§€ê°€ ìˆì„ ë•Œë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
    ì´ë¯¸ì§€ URL ë˜ëŠ” ë¡œì»¬ íŒŒì¼ ê²½ë¡œë¥¼ ë°›ì•„ Google Lensë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.

    Args:
        image_source: ì´ë¯¸ì§€ URL ë˜ëŠ” ë¡œì»¬ íŒŒì¼ ê²½ë¡œ (í•„ìˆ˜)

    Returns:
        Google ì´ë¯¸ì§€ ê²€ìƒ‰ ê²°ê³¼ + ë¸”ë¡œê·¸ ë³¸ë¬¸
    """
    # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ writer íšë“
    writer = get_stream_writer()


    if not image_source or not image_source.strip():
        return "[ì´ë¯¸ì§€ ì—†ìŒ] ì´ ë„êµ¬ëŠ” ìƒˆ ì´ë¯¸ì§€ê°€ ìˆì„ ë•Œë§Œ ì‚¬ìš©í•˜ì„¸ìš”."

    image_source = image_source.strip()

    if not image_source.startswith(('http://', 'https://', '/')):
        return "[ì´ë¯¸ì§€ ì—†ìŒ] ìœ íš¨í•œ ì´ë¯¸ì§€ ê²½ë¡œê°€ ì•„ë‹™ë‹ˆë‹¤."

    if not image_source.startswith(('http://', 'https://')) and not os.path.exists(image_source):
        return f"[ì´ë¯¸ì§€ ì—†ìŒ] íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_source}"

    searcher = get_searcher()

    # ğŸ”¥ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸: ì´ë¯¸ì§€ ì—…ë¡œë“œ ì‹œì‘
    writer({"tool": "search_food_by_image", "status": "ì´ë¯¸ì§€ ì—…ë¡œë“œ ì¤‘..."})

    image_url = searcher.get_image_url(image_source)
    if not image_url:
        return f"ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_source}"


    # ğŸ”¥ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸: Google Lens ê²€ìƒ‰ ì‹œì‘
    writer({"tool": "search_food_by_image", "status": "Google Lensë¡œ ê²€ìƒ‰ ì¤‘..."})

    result = searcher.search_with_combined(image_url)

    if "error" in result:
        return f"ê²€ìƒ‰ ì‹¤íŒ¨: {result['error']}"


    # ğŸ”¥ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸: ê²€ìƒ‰ ì™„ë£Œ
    writer({"tool": "search_food_by_image", "status": "ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„ ì¤‘..."})

    output = []
    blog_links = []
    thumbnails = []

    visual = result.get("visual_matches", [])
    if visual:
        output.append("[ê²€ìƒ‰ ê²°ê³¼]")
        for i, v in enumerate(visual[:10], 1):
            title = v.get("title", "")
            snippet = v.get("snippet", "")
            link = v.get("link", "")
            thumbnail = v.get("thumbnail", "")

            if title:
                line = f"{i}. {title}"
                if snippet:
                    line += f" - {snippet[:100]}"
                output.append(line)

            if thumbnail and len(thumbnails) < 3:
                thumbnails.append(thumbnail)

            if link and ('blog.naver.com' in link or 'tistory.com' in link):
                blog_links.append(link)

    if thumbnails:
        output.append("\n[ê²€ìƒ‰ ê²°ê³¼ ì´ë¯¸ì§€]")
        for url in thumbnails:
            output.append(f"[IMAGE:{url}]")

    if blog_links:
        output.append("\n[ë¸”ë¡œê·¸ ë³¸ë¬¸ (ë©”ë‰´ íŒë‹¨ ì°¸ê³ ìš©)]")
        for i, link in enumerate(blog_links[:3], 1):
            blog_data = extract_blog_content(link)
            if blog_data["content"]:
                output.append(f"\n--- ë¸”ë¡œê·¸ {i} ---")
                output.append(blog_data["content"][:1000])

    texts = result.get("text", [])
    if texts:
        text_list = [t.get("text", "") for t in texts[:5] if t.get("text")]
        if text_list:
            output.append(f"\n[ì´ë¯¸ì§€ í…ìŠ¤íŠ¸] {', '.join(text_list)}")

    output.append("\n[íŒë‹¨ ìš”ì²­]")
    output.append("1. ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ ê²°ê³¼ ì œëª©, ë¸”ë¡œê·¸ ë³¸ë¬¸ì„ ì°¸ê³ í•˜ì„¸ìš”.")
    output.append("2. ìŒì‹ ì´ë¦„ë§Œ ë¬¼ì–´ë³´ë©´: '~ë¡œ ë³´ì…ë‹ˆë‹¤' + ì‹ë‹¹ì´ ë³´ì´ë©´ 'í˜¹ì‹œ OOì—ì„œ ë“œì…¨ë‚˜ìš”?'")
    output.append("3. ì‹ë‹¹/ë©”ë‰´ëª…ê¹Œì§€ ë¬¼ì–´ë³´ë©´: ê°€ëŠ¥ì„± ìˆëŠ” ì‹ë‹¹ 2~3ê³³ì„ í›„ë³´ë¡œ ë‚˜ì—´í•˜ì„¸ìš”.")

    return "\n".join(output) if output else "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"
